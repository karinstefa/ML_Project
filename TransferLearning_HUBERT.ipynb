{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using the device cpu - cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cuda_id = 2\n",
    "device = torch.device(\"cuda:%s\" % cuda_id if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name(cuda_id) if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"We are using the device %s - %s\" % (device, device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=[]\n",
    "labels=[]\n",
    "\n",
    "for dirname, _, filenames in os.walk('Emotions/input/'):\n",
    "    for filename in filenames:\n",
    "        paths.append(os.path.join(dirname, filename))\n",
    "        label = filename[::-1].split('.')[0][::-1]\n",
    "        name = dirname[::].split('/')[2]\n",
    "        name_complete = name+'.'+label\n",
    "        labels.append(name_complete.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>Emotions/input/Disgusted\\03-01-07-02-02-02-20.wav</td>\n",
       "      <td>disgusted.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Emotions/input/Angry\\03-01-05-01-02-01-02.wav</td>\n",
       "      <td>angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>Emotions/input/Angry\\YAF_chief_angry.wav</td>\n",
       "      <td>angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>Emotions/input/Happy\\03-01-03-02-01-01-10.wav</td>\n",
       "      <td>happy.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10273</th>\n",
       "      <td>Emotions/input/Sad\\03-02-04-01-01-02-21.wav</td>\n",
       "      <td>sad.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>Emotions/input/Angry\\1072_IWW_ANG_XX.wav</td>\n",
       "      <td>angry.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>Emotions/input/Neutral\\03-01-01-01-02-01-03.wav</td>\n",
       "      <td>neutral.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4664</th>\n",
       "      <td>Emotions/input/Fearful\\1019_MTI_FEA_XX.wav</td>\n",
       "      <td>fearful.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>Emotions/input/Fearful\\03-02-06-01-02-01-01.wav</td>\n",
       "      <td>fearful.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10644</th>\n",
       "      <td>Emotions/input/Sad\\1017_IWL_SAD_XX.wav</td>\n",
       "      <td>sad.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  speech          label\n",
       "2354   Emotions/input/Disgusted\\03-01-07-02-02-02-20.wav  disgusted.wav\n",
       "49         Emotions/input/Angry\\03-01-05-01-02-01-02.wav      angry.wav\n",
       "1989            Emotions/input/Angry\\YAF_chief_angry.wav      angry.wav\n",
       "6182       Emotions/input/Happy\\03-01-03-02-01-01-10.wav      happy.wav\n",
       "10273        Emotions/input/Sad\\03-02-04-01-01-02-21.wav        sad.wav\n",
       "1375            Emotions/input/Angry\\1072_IWW_ANG_XX.wav      angry.wav\n",
       "8291     Emotions/input/Neutral\\03-01-01-01-02-01-03.wav    neutral.wav\n",
       "4664          Emotions/input/Fearful\\1019_MTI_FEA_XX.wav    fearful.wav\n",
       "4268     Emotions/input/Fearful\\03-02-06-01-02-01-01.wav    fearful.wav\n",
       "10644             Emotions/input/Sad\\1017_IWL_SAD_XX.wav        sad.wav"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'speech':paths,'label':labels})\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.label.apply(lambda x: x != '.ds_store')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelencoder para convertir las clases a etiquetas númericas\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "# Encode labels in column 'label'.\n",
    "data['label'] = data['label'].apply(lambda x: str(x).split('.wav')[0])\n",
    "data['label']= label_encoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waveplot(data,sr,emotion):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.title(emotion,size=20)\n",
    "    librosa.display.waveshow(data,sr=sr)\n",
    "    plt.show()\n",
    "    \n",
    "def spectogram(data,sr,emotion):\n",
    "    x=librosa.stft(data)\n",
    "    xdb=librosa.amplitude_to_db(abs(x))\n",
    "    plt.figure(figsize=(11,4))\n",
    "    librosa.display.specshow(xdb,sr=sr,x_axis='time',y_axis='hz')\n",
    "    plt.colorbar()\n",
    "    \n",
    "def ana_emotion(emotion:str, df:pd.DataFrame):\n",
    "    path=np.array(df['speech'][df['label']==emotion])[0]\n",
    "    data,sampling_rate=librosa.load(path)\n",
    "    waveplot(data,sampling_rate,emotion)\n",
    "    spectogram(data,sampling_rate,emotion)\n",
    "    Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clases de emociones en el datset\n",
    "data['label'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in data['label'].unique().tolist(): # ver uno de cada sentimiento\n",
    "#    ana_emotion(i,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes Septrales - Transformación del audio en una serie de parametros que representan de forma compacta el sonido\n",
    "def MFCC(filename):\n",
    "    y, sr = librosa.load(filename,duration=3,offset=0.5)\n",
    "    return np.mean(librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40).T,axis=0)\n",
    "\n",
    "mfcc= data['speech'].apply(lambda x:MFCC(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de Registros de Audio: (12798,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Cantidad de Registros de Audio: {mfcc.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.asarray(mfcc.to_list())\n",
    "y = np.asarray(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11518\n",
      "11518\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "1280\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([1946, 1675, 1831, 1945, 1620, 1954,  547], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]),\n",
       " array([221, 188, 216, 222, 175, 213,  45], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/hubert-large-superb-er\")\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[-5.6967,  2.4605,  0.2270,  ...,  0.0762,  0.0358,  0.0757],\n",
       "        [-5.8255,  2.0707, -0.0169,  ...,  0.1148,  0.0684,  0.0758],\n",
       "        [-5.8321,  2.1958,  0.2574,  ...,  0.1160,  0.0416,  0.0864],\n",
       "        ...,\n",
       "        [-5.6735,  2.5914,  0.2032,  ...,  0.0816,  0.0519,  0.0960],\n",
       "        [-5.3521,  2.9141, -0.6245,  ...,  0.1363,  0.1076,  0.1759],\n",
       "        [-5.6563,  2.6548,  0.2313,  ...,  0.1415,  0.1053,  0.1135]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings = feature_extractor(list(X_train), sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6967,  2.4605,  0.2270,  ...,  0.0762,  0.0358,  0.0757],\n",
       "        [-5.8255,  2.0707, -0.0169,  ...,  0.1148,  0.0684,  0.0758],\n",
       "        [-5.8321,  2.1958,  0.2574,  ...,  0.1160,  0.0416,  0.0864],\n",
       "        ...,\n",
       "        [-5.6735,  2.5914,  0.2032,  ...,  0.0816,  0.0519,  0.0960],\n",
       "        [-5.3521,  2.9141, -0.6245,  ...,  0.1363,  0.1076,  0.1759],\n",
       "        [-5.6563,  2.6548,  0.2313,  ...,  0.1415,  0.1053,  0.1135]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['input_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[-0.3613, -0.9039, -0.9039,  ..., -0.3613, -1.4465,  0.1814]],\n",
       "       dtype=torch.float64), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encodings = feature_extractor(y_train, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "test_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class EmotionDataset(torch.utils.data.Dataset):\n",
    "#    def __init__(self, encodings, labels):\n",
    "#        self.encodings = encodings\n",
    "#        pattern = {1:0, 2:0, 3:1, 4:3, 5:2}\n",
    "#        self.labels = [pattern[x] for x in labels]\n",
    "#\n",
    "#    def __getitem__(self, idx):\n",
    "#        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "#        item['labels'] = torch.tensor(self.labels[idx])\n",
    "#        return item\n",
    "#\n",
    "#    def __len__(self):\n",
    "#        return len(self.labels)\n",
    "#    \n",
    "#\n",
    "#train_dataset = EmotionDataset(train_encodings, list(X_train[labels]))\n",
    "#test_dataset = EmotionDataset(test_encodings, list(y_train[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HubertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Loading the model\n",
    "model = HubertForSequenceClassification.from_pretrained(\"superb/hubert-large-superb-er\")\n",
    "model.to(device)\n",
    "\n",
    "# Loading the optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(outputs):\n",
    "    probabilities = torch.softmax(outputs[\"logits\"], dim=1)\n",
    "    predictions = torch.argmax(probabilities, dim=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set the number of epoch\n",
    "epoch = 2\n",
    "\n",
    "# Start training\n",
    "model.train()\n",
    "\n",
    "train_loss = list()\n",
    "train_accuracies = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/720 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4a46bb770d46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(epoch):\n",
    "    print('Epoch %s/%s' % (epoch_i + 1, epoch))\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    # Get training data by DataLoader\n",
    "    train_loader = DataLoader(X_train, batch_size=16, shuffle=True)\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    epoch_loss = list()\n",
    "    pbar = tqdm(train_loader)\n",
    "\n",
    "    for batch in pbar:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # make predictions\n",
    "        predictions = predict(outputs)\n",
    "\n",
    "        # count accuracy\n",
    "        correct += predictions.eq(labels).sum().item()\n",
    "        count += len(labels)\n",
    "        accuracy = correct * 1.0 / count\n",
    "\n",
    "        # show progress along with metrics\n",
    "        pbar.set_postfix({\n",
    "            'Loss': '{:.3f}'.format(loss.item()),\n",
    "            'Accuracy': '{:.3f}'.format(accuracy)\n",
    "        })\n",
    "        \n",
    "        # record the loss for each batch\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    pbar.close()\n",
    "    \n",
    "    # record the loss and accuracy for each epoch\n",
    "    train_loss += epoch_loss\n",
    "    train_accuracies.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the training loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbyklEQVR4nO3df5yVdZ338dfbAcFERQETGBTYSEI09DGRobuBmQvoim1ZupS/HrtIG1mYK6R1595ut2R3aWyWq7uWPvyVrWtxJ4XpqpSt6aBAEZKEGCOoQPErMwU+9x/nO3QYz8yc+Z45cwbn/Xw8zmPOdV3f67o+37ngvOf6XuecSxGBmZlZR+1X6wLMzGzf5AAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4Qe9OTtEPSyFrXUU2Spkt6oLPbmrXFAWJVJWmtpFPT8wsk/bTK+3tE0t8Xz4uIfhGxppr7zSHpxhRuOyS9Jun1oukfdmRbEXFHRJzW2W07qvh425ufA8T2GZJ61bqGzhQRM1O49QP+D/Cd5umImNLc7s3Wb3vzcIBYl5D0DuBG4D3pL+wtaX4fSf9X0m8lvZT+Kj8gLZsoqUnSHEkvAt+SdKikH0jaKOn36Xl9av9F4C+Br6d9fD3ND0lvS88PkXRbWv95SZ+TtF9adoGkn6Z6fi/pOUlTWvYltZ0r6T9bzPuapPlF21ojaXvazvQO/r7Wpn4vB/4gqVfa52/SNn8l6QNF7fc6u0t9ninp2dSXGyQpo22dpK9I2pT6MSu171CopeN8vaT16XG9pD5p2cB0HLdI+p2knxQdkzmSXkh9XiXpfR3Zr1WXA8S6RESsBGYC/5P+wu6fFn0JeDswDngbMBT4X0WrHgEcBhwFzKDwb/ZbafpI4I/A19M+rgR+AsxK+5hVopR/BQ4BRgLvBc4DLixa/m5gFTAQuBb4j+YX0xbuAqZKOhgKL7TAh4E7JR0IzAemRMRBwARgaXu/oxLOBU4H+kfETuA3FALyEOCfgdslDW5j/TOAdwHvTLX9dUbbfwCmUDg+JwBnZfQD4ErgxLSddwLjgc+lZZ8BmoBBwFuBK4CQdDQwC3hX+j3+NbA2c/9WBQ4Qq5n0wvwPwOyI+F1EbKcwlHNOUbPdwBci4k8R8ceI2BwR90bEK6n9FykEQTn7qwM+Anw2IrZHxFrgK8DHipo9HxE3R8Qu4FZgMIUXtb1ExPPAU/z5BfUU4JWIeLyo7rGSDoiIDRGxopwaW5gfEesi4o9pn9+NiPURsTsivgM8S+GFuDXzImJLRPwWeJjCi3dH234Y+FpENEXE74F5Gf0AmA7874h4OSI2UgjA5t/76xR+z0dFxOsR8ZMofEnfLqAPMEZS74hYGxG/ydy/VYEDxGppEPAWYEkavtgC/CjNb7YxIl5tnpD0Fkn/loaftgGLgf4pHNozENgfeL5o3vMUznqavdj8JCJeSU/7tbK9OymcJQD8XZomIv5AIahmAhsk3S9pdBn1tbSueELSeZKWFv2uxqY+tebFouev0Ho/2mo7pEUde9XUAUN44+99SHr+ZWA18EAa9psLEBGrgU8DVwEvS7pb0hCs23CAWFdq+dXPmygMQR0TEf3T45B0Ubm1dT4DHA28OyIOBv4qzVcr7Vvu73UKw1/NjgRe6EAfin0XmJiuwXyAFCAAEbEoIt5P4S/rZ4CbM7a/py+SjkrbmAUMSEOAv+TP/a6WDUB90fSwzO2s542/9/UA6WzwMxExEvgb4NLmax0RcWdEnJzWDQpDntZNOECsK70E1EvaHyAidlN4UbxO0uEAkoZKamus/iAKobNF0mHAF0rso+RnPtKw1D3AFyUdlF6ULwVuz+lMGop5hMI1mefSdR4kvVXSmelayJ+AHRSGYypxIIUX0I1pHxdSOAOptnuAT6Xj0h+YU8Y6vSX1LXr0onDN6HOSBkkaSOE61+0Aks6Q9LY0pLmNwu9ql6SjJZ2SLra/SuG4V/p7tE7kALGu9N/ACuBFSZvSvDkUhi8eT0NSD1I4w2jN9cABFM4mHqcw5FXsa8CH0ruJ5pdY/5PAH4A1wE8pnDXcktWbgjuBUyk6+6Dw/+ozFP7C/h2FazT/WME+iIhfUbhe8z8UQvJY4LFKtlmmm4EHgOXA08BCYCdtv5AvpPBi3/y4CvgXoDFt5xcUrh/9S2o/isJx30Ghf9+IiEcoXP+YR+FYvwgcTuECu3UT8g2lzKxc6W3NN0bEUe02tjc9n4GYWaskHSBpavocylAKQ4b31bou6x58BmJmrZL0FuBRYDSF4aj7gU9FxLaaFmbdggPEzMyyeAjLzMyy9KgvaRs4cGAMHz681mWYme1TlixZsikiBrWc36MCZPjw4TQ2Nta6DDOzfYqk50vN9xCWmZllcYCYmVkWB4iZmWXpUddAzKx7ef3112lqauLVV19tv7FVXd++famvr6d3795ltXeAmFnNNDU1cdBBBzF8+HBK37fLukpEsHnzZpqamhgxYkRZ63gIy8xq5tVXX2XAgAEOj25AEgMGDOjQ2aADxMxqyuHRfXT0WDhAzMwsiwPEzHqszZs3M27cOMaNG8cRRxzB0KFD90y/9tprba7b2NjIJZdc0u4+JkyY0Cm1PvLII5xxxhmdsq3O4ovoZtZjDRgwgKVLlwJw1VVX0a9fPy677LI9y3fu3EmvXqVfJhsaGmhoaGh3Hz/72c86pdbuyGcgZmZFLrjgAi699FImTZrEnDlzeOKJJ5gwYQLHH388EyZMYNWqVcDeZwRXXXUVF110ERMnTmTkyJHMn//nm2H269dvT/uJEyfyoQ99iNGjRzN9+nSavw194cKFjB49mpNPPplLLrmkQ2cad911F8ceeyxjx45lzpzCHYd37drFBRdcwNixYzn22GO57rrrAJg/fz5jxozhuOOO45xzzqn4d+UzEDPrFv75/63gV+s79zYjY4YczBf+5pgOr/frX/+aBx98kLq6OrZt28bixYvp1asXDz74IFdccQX33nvvG9Z55plnePjhh9m+fTtHH300H//4x9/weYqnn36aFStWMGTIEE466SQee+wxGhoauPjii1m8eDEjRozg3HPPLbvO9evXM2fOHJYsWcKhhx7Kaaedxve+9z2GDRvGCy+8wC9/+UsAtmzZAsC8efN47rnn6NOnz555lfAZiJlZC2effTZ1dXUAbN26lbPPPpuxY8cye/ZsVqxYUXKd008/nT59+jBw4EAOP/xwXnrppTe0GT9+PPX19ey3336MGzeOtWvX8swzzzBy5Mg9n73oSIA8+eSTTJw4kUGDBtGrVy+mT5/O4sWLGTlyJGvWrOGTn/wkP/rRjzj44IMBOO6445g+fTq33357q0NzHeEzEDPrFnLOFKrlwAMP3PP885//PJMmTeK+++5j7dq1TJw4seQ6ffr02fO8rq6OnTt3ltWmkpv6tbbuoYceyrJly1i0aBE33HAD99xzD7fccgv3338/ixcvZsGCBVx99dWsWLGioiDxGYiZWRu2bt3K0KFDAfj2t7/d6dsfPXo0a9asYe3atQB85zvfKXvdd7/73Tz66KNs2rSJXbt2cdddd/He976XTZs2sXv3bj74wQ9y9dVX89RTT7F7927WrVvHpEmTuPbaa9myZQs7duyoqHafgZiZteHyyy/n/PPP56tf/SqnnHJKp2//gAMO4Bvf+AaTJ09m4MCBjB8/vtW2Dz30EPX19Xumv/vd73LNNdcwadIkIoKpU6cybdo0li1bxoUXXsju3bsBuOaaa9i1axcf/ehH2bp1KxHB7Nmz6d+/f0W196h7ojc0NIRvKGXWfaxcuZJ3vOMdtS6j5nbs2EG/fv2ICD7xiU8watQoZs+eXZNaSh0TSUsi4g3vWfYQlplZjd18882MGzeOY445hq1bt3LxxRfXuqSyeAjLzKzGZs+eXbMzjkr4DMTMaqonDaN3dx09Fg4QM6uZvn37snnzZodIN9B8P5C+ffuWvY6HsMysZurr62lqamLjxo21LsX48x0Jy+UAMbOa6d27d9l3v7Pux0NYZmaWxQFiZmZZahogkiZLWiVptaS5JZZL0vy0fLmkE1osr5P0tKQfdF3VZmYGNQwQSXXADcAUYAxwrqQxLZpNAUalxwzgmy2WfwpYWeVSzcyshFqegYwHVkfEmoh4DbgbmNaizTTgtih4HOgvaTCApHrgdODfu7JoMzMrqGWADAXWFU03pXnltrkeuBzY3dZOJM2Q1Cip0W8VNDPrPLUMEJWY1/LTRCXbSDoDeDkilrS3k4i4KSIaIqJh0KBBOXWamVkJtQyQJmBY0XQ9sL7MNicBZ0paS2Ho6xRJt1evVDMza6mWAfIkMErSCEn7A+cAC1q0WQCcl96NdSKwNSI2RMRnI6I+Ioan9f47Ij7apdWbmfVwNfskekTslDQLWATUAbdExApJM9PyG4GFwFRgNfAKcGGt6jUzs735hlJmZtYm31DKzMw6lQPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMstQ0QCRNlrRK0mpJc0ssl6T5aflySSek+cMkPSxppaQVkj7V9dWbmfVsNQsQSXXADcAUYAxwrqQxLZpNAUalxwzgm2n+TuAzEfEO4ETgEyXWNTOzKqrlGch4YHVErImI14C7gWkt2kwDbouCx4H+kgZHxIaIeAogIrYDK4GhXVm8mVlPV8sAGQqsK5pu4o0h0G4bScOB44Gfd36JZmbWmloGiErMi460kdQPuBf4dERsK7kTaYakRkmNGzduzC7WzMz2VssAaQKGFU3XA+vLbSOpN4XwuCMi/qu1nUTETRHREBENgwYN6pTCzcystgHyJDBK0ghJ+wPnAAtatFkAnJfejXUisDUiNkgS8B/Ayoj4ateWbWZmAL1qteOI2ClpFrAIqANuiYgVkmam5TcCC4GpwGrgFeDCtPpJwMeAX0hamuZdERELu7ALZmY9miJaXnZ482poaIjGxsZal2Fmtk+RtCQiGlrO9yfRzcwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyylBUgkg6UtF96/nZJZ0rqXd3SzMysOyv3DGQx0FfSUOAh4ELg29UqyszMur9yA0QR8Qrwt8C/RsQHgDHVK8vMzLq7sgNE0nuA6cD9aV6v6pRkZmb7gnID5NPAZ4H7ImKFpJHAw1WryszMur2yAiQiHo2IMyPiS+li+qaIuKTSnUuaLGmVpNWS5pZYLknz0/Llkk4od10zM6uuct+FdaekgyUdCPwKWCXpnyrZsaQ64AZgCoXrKedKanldZQowKj1mAN/swLpmZlZF5Q5hjYmIbcBZwELgSOBjFe57PLA6ItZExGvA3cC0Fm2mAbdFweNAf0mDy1zXzMyqqNwA6Z0+93EW8P2IeB2ICvc9FFhXNN2U5pXTppx1AZA0Q1KjpMaNGzdWWLKZmTUrN0D+DVgLHAgslnQUsK3CfavEvJah1FqbctYtzIy4KSIaIqJh0KBBHSzRzMxaU9ZbcSNiPjC/aNbzkiZVuO8mYFjRdD2wvsw2+5exrpmZVVG5F9EPkfTV5qEgSV+hcDZSiSeBUZJGSNofOAdY0KLNAuC89G6sE4GtEbGhzHXNzKyKyh3CugXYDnw4PbYB36pkxxGxE5gFLAJWAvekz5jMlDQzNVsIrAFWAzcD/9jWupXUY2ZmHaOI9q+FS1oaEePam9fdNTQ0RGNjY63LMDPbp0haEhENLeeXewbyR0knF23sJOCPnVWcmZnte8r9PquZwG2SDknTvwfOr05JZma2Lyj3XVjLgHdKOjhNb5P0aWB5FWszM7NurEN3JIyIbekT6QCXVqEeMzPbR1RyS9tSH+YzM7MeopIAqfSrTMzMbB/W5jUQSdspHRQCDqhKRWZmtk9oM0Ai4qCuKsTMzPYtlQxhmZlZD+YAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsNQkQSYdJ+rGkZ9PPQ1tpN1nSKkmrJc0tmv9lSc9IWi7pPkn9u6x4MzMDancGMhd4KCJGAQ+l6b1IqgNuAKYAY4BzJY1Ji38MjI2I44BfA5/tkqrNzGyPWgXINODW9PxW4KwSbcYDqyNiTUS8Btyd1iMiHoiInand40B9dcs1M7OWahUgb42IDQDp5+El2gwF1hVNN6V5LV0E/LDTKzQzszb1qtaGJT0IHFFi0ZXlbqLEvGixjyuBncAdbdQxA5gBcOSRR5a5azMza0/VAiQiTm1tmaSXJA2OiA2SBgMvl2jWBAwrmq4H1hdt43zgDOB9ERG0IiJuAm4CaGhoaLWdmZl1TK2GsBYA56fn5wPfL9HmSWCUpBGS9gfOSeshaTIwBzgzIl7pgnrNzKyFWgXIPOD9kp4F3p+mkTRE0kKAdJF8FrAIWAncExEr0vpfBw4CfixpqaQbu7oDZmY9XdWGsNoSEZuB95WYvx6YWjS9EFhYot3bqlqgmZm1y59ENzOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLDUJEEmHSfqxpGfTz0NbaTdZ0ipJqyXNLbH8MkkhaWD1qzYzs2K1OgOZCzwUEaOAh9L0XiTVATcAU4AxwLmSxhQtHwa8H/htl1RsZmZ7qVWATANuTc9vBc4q0WY8sDoi1kTEa8Ddab1m1wGXA1HFOs3MrBW1CpC3RsQGgPTz8BJthgLriqab0jwknQm8EBHL2tuRpBmSGiU1bty4sfLKzcwMgF7V2rCkB4EjSiy6stxNlJgXkt6StnFaORuJiJuAmwAaGhp8tmJm1kmqFiARcWpryyS9JGlwRGyQNBh4uUSzJmBY0XQ9sB74C2AEsExS8/ynJI2PiBc7rQNmZtamWg1hLQDOT8/PB75fos2TwChJIyTtD5wDLIiIX0TE4RExPCKGUwiaExweZmZdq1YBMg94v6RnKbyTah6ApCGSFgJExE5gFrAIWAncExEralSvmZm1ULUhrLZExGbgfSXmrwemFk0vBBa2s63hnV2fmZm1z59ENzOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy+IAMTOzLA4QMzPL4gAxM7MsDhAzM8viADEzsywOEDMzy6KIqHUNXUbSRuD5WteRYSCwqdZFdKGe1l9wn3uKfbXPR0XEoJYze1SA7KskNUZEQ63r6Co9rb/gPvcUb7Y+ewjLzMyyOEDMzCyLA2TfcFOtC+hiPa2/4D73FG+qPvsaiJmZZfEZiJmZZXGAmJlZFgdINyDpMEk/lvRs+nloK+0mS1olabWkuSWWXyYpJA2sftWVqbTPkr4s6RlJyyXdJ6l/lxXfQWUcN0man5Yvl3RCuet2V7l9ljRM0sOSVkpaIelTXV99nkqOc1peJ+lpST/ouqorFBF+1PgBXAvMTc/nAl8q0aYO+A0wEtgfWAaMKVo+DFhE4YOSA2vdp2r3GTgN6JWef6nU+t3h0d5xS22mAj8EBJwI/Lzcdbvjo8I+DwZOSM8PAn79Zu9z0fJLgTuBH9S6P+U+fAbSPUwDbk3PbwXOKtFmPLA6ItZExGvA3Wm9ZtcBlwP7yrsiKupzRDwQETtTu8eB+uqWm62940aavi0KHgf6Sxpc5rrdUXafI2JDRDwFEBHbgZXA0K4sPlMlxxlJ9cDpwL93ZdGVcoB0D2+NiA0A6efhJdoMBdYVTTeleUg6E3ghIpZVu9BOVFGfW7iIwl923VE5fWitTbn9724q6fMekoYDxwM/7/wSO12lfb6ewh+Au6tUX1X0qnUBPYWkB4EjSiy6stxNlJgXkt6StnFabm3VUq0+t9jHlcBO4I6OVddl2u1DG23KWbc7qqTPhYVSP+Be4NMRsa0Ta6uW7D5LOgN4OSKWSJrY2YVVkwOki0TEqa0tk/RS8+l7OqV9uUSzJgrXOZrVA+uBvwBGAMskNc9/StL4iHix0zqQoYp9bt7G+cAZwPsiDSJ3Q232oZ02+5exbndUSZ+R1JtCeNwREf9VxTo7UyV9/hBwpqSpQF/gYEm3R8RHq1hv56j1RRg/AuDL7H1B+doSbXoBayiERfNFumNKtFvLvnERvaI+A5OBXwGDat2XdvrZ7nGjMPZdfHH1iY4c8+72qLDPAm4Drq91P7qqzy3aTGQfuohe8wL8CIABwEPAs+nnYWn+EGBhUbupFN6V8hvgyla2ta8ESEV9BlZTGE9emh431rpPbfT1DX0AZgIz03MBN6TlvwAaOnLMu+Mjt8/AyRSGfpYXHdupte5PtY9z0Tb2qQDxV5mYmVkWvwvLzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzDJI2pF+Dpf0d5287StaTP+sM7dv1lkcIGaVGQ50KEAk1bXTZK8AiYgJHazJrEs4QMwqMw/4S0lLJc1O93T4sqQn0z0fLgaQNDHd5+JOCh8iQ9L3JC1J972YkebNAw5I27sjzWs+21Ha9i8l/ULSR4q2/Yik/0z3SLlD6XttzKrJ34VlVpm5wGURcQZACoKtEfEuSX2AxyQ9kNqOB8ZGxHNp+qKI+J2kA4AnJd0bEXMlzYqIcSX29bfAOOCdwMC0zuK07HjgGArfrfQYcBLw087urFkxn4GYda7TgPMkLaXwNeQDgFFp2RNF4QFwiaRlFO5nMqyoXWtOBu6KiF0R8RLwKPCuom03RcRuCl//MbwT+mLWJp+BmHUuAZ+MiEV7zSx8TfcfWkyfCrwnIl6R9AiFb2Jtb9ut+VPR8134/7Z1AZ+BmFVmO4VbrzZbBHw8fSU5kt4u6cAS6x0C/D6Fx2gK387a7PXm9VtYDHwkXWcZBPwV8ESn9MIsg/9KMavMcmBnGor6NvA1CsNHT6UL2RspfbveHwEzJS0HVlEYxmp2E7Bc0lMRMb1o/n3Aeyh8VXgAl0fEiymAzLqcv43XzMyyeAjLzMyyOEDMzCyLA8TMzLI4QMzMLIsDxMzMsjhAzMwsiwPEzMyy/H8x7D86SHR4FwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot Iteration vs Training Loss\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Iteration vs Training Loss\")  \n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Epoch vs Training Accuracy\n",
    "acc_X = np.arange(len(train_accuracies))+1                          \n",
    "plt.plot(acc_X, train_accuracies,\"-\", label=\"Training Accuracy\")\n",
    "plt.xticks(acc_X)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epoch vs Training Accuracy\")  \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11518 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-cc011e5fcc45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get test data by DataLoader\n",
    "test_loader = DataLoader(X_train, batch_size=1, shuffle=False)\n",
    "\n",
    "# Start testing\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    count = 0\n",
    "    record = {\"labels\":list(), \"predictions\":list()}\n",
    "    \n",
    "    pbar = tqdm(test_loader)\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # make predictions\n",
    "        predictions = predict(outputs)\n",
    "\n",
    "        # count accuracy\n",
    "        correct += predictions.eq(labels).sum().item()\n",
    "        count += len(labels)\n",
    "        accuracy = correct * 1.0 / count\n",
    "\n",
    "        # show progress along with metrics\n",
    "        pbar.set_postfix({\n",
    "            'loss': '{:.3f}'.format(loss.item()),\n",
    "            'accuracy': '{:.3f}'.format(accuracy)\n",
    "        })\n",
    "    \n",
    "        # record the results\n",
    "        record[\"labels\"] += labels.cpu().numpy().tolist()\n",
    "        record[\"predictions\"] += predictions.cpu().numpy().tolist()\n",
    "        \n",
    "    pbar.close()\n",
    "    \n",
    "time.sleep(0.3)\n",
    "print(\"The final accuracy on the test dataset: %s%%\" % round(accuracy*100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
